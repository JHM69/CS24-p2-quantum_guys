package com.quantum_guys.dncc_eco_sync.face;import android.Manifest;import android.annotation.SuppressLint;import android.app.Activity;import android.content.Context;import android.content.Intent;import android.content.SharedPreferences;import android.content.pm.PackageManager;import android.content.res.AssetFileDescriptor;import android.graphics.Bitmap;import android.graphics.BitmapFactory;import android.graphics.Canvas;import android.graphics.Color;import android.graphics.ImageFormat;import android.graphics.Matrix;import android.graphics.Paint;import android.graphics.Rect;import android.graphics.RectF;import android.graphics.YuvImage;import android.media.Image;import android.net.Uri;import android.os.Build;import android.os.Bundle;import android.os.ParcelFileDescriptor;import android.util.Log;import android.util.Size;import android.view.View;import android.widget.Button;import android.widget.ImageView;import android.widget.ProgressBar;import android.widget.TextView;import android.widget.Toast;import androidx.annotation.NonNull;import androidx.annotation.RequiresApi;import androidx.appcompat.app.AppCompatActivity;import androidx.camera.core.CameraSelector;import androidx.camera.core.ImageAnalysis;import androidx.camera.core.Preview;import androidx.camera.lifecycle.ProcessCameraProvider;import androidx.camera.view.PreviewView;import androidx.core.content.ContextCompat;import com.afollestad.materialdialogs.MaterialDialog;import com.google.common.util.concurrent.ListenableFuture;import com.google.gson.Gson;import com.google.gson.reflect.TypeToken;import com.google.mlkit.vision.common.InputImage;import com.google.mlkit.vision.face.Face;import com.google.mlkit.vision.face.FaceDetector;import com.google.mlkit.vision.face.FaceDetectorOptions;import com.quantum_guys.dncc_eco_sync.MainActivity;import com.quantum_guys.dncc_eco_sync.R;import com.quantum_guys.dncc_eco_sync.global.UserDB;import com.quantum_guys.dncc_eco_sync.model.User;import com.quantum_guys.dncc_eco_sync.retrofit.ApiUtils;import com.quantum_guys.dncc_eco_sync.retrofit.AuthService;import org.jetbrains.annotations.NotNull;import org.tensorflow.lite.Interpreter;import java.io.ByteArrayOutputStream;import java.io.FileDescriptor;import java.io.FileInputStream;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.ByteOrder;import java.nio.MappedByteBuffer;import java.nio.ReadOnlyBufferException;import java.nio.channels.FileChannel;import java.util.ArrayList;import java.util.HashMap;import java.util.Map;import java.util.Objects;import java.util.concurrent.ExecutionException;import java.util.concurrent.Executor;import java.util.concurrent.Executors;import es.dmoral.toasty.Toasty;import retrofit2.Call;import retrofit2.Response;public class AddFace extends AppCompatActivity {    FaceDetector detector;    SimilarityClassifier.Recognition result;    private ListenableFuture<ProcessCameraProvider> cameraProviderFuture;    PreviewView previewView;    ImageView face_preview, camera_switch, add_photo;    Interpreter tfLite;    TextView reco_name, preview_info;    Button recognize, add_face;    CameraSelector cameraSelector;    boolean start = true, flipX = false;    Context context = AddFace.this;    int cam_face = CameraSelector.LENS_FACING_FRONT;    User user;   // DatabaseReference db;    ProgressBar pBar;    int[] intValues;    int inputSize = 112;  //Input size for model    boolean isModelQuantized = false;    float[][] embeedings;    float IMAGE_MEAN = 128.0f;    float IMAGE_STD = 128.0f;    int OUTPUT_SIZE = 192; //Output size of model    private static final int SELECT_PICTURE = 1;    ProcessCameraProvider cameraProvider;    private static final int MY_CAMERA_REQUEST_CODE = 100;    String modelFile = "mobile_face_net.tflite"; //model name    private HashMap<String, SimilarityClassifier.Recognition> registered = new HashMap<>(); //saved Faces    @SuppressLint("SetTextI18n")    @RequiresApi(api = Build.VERSION_CODES.M)    @Override    protected void onCreate(Bundle savedInstanceState) {        super.onCreate(savedInstanceState);        registered = readFromSP(); //Load saved faces from memory when app starts        setContentView(R.layout.driver_add_face);        face_preview = findViewById(R.id.imageView);        reco_name = findViewById(R.id.textView);        recognize = findViewById(R.id.button3);        camera_switch = findViewById(R.id.button5);        add_photo = findViewById(R.id.button6);        add_face = findViewById(R.id.button2);        preview_info = findViewById(R.id.textView2);        pBar = findViewById(R.id.progressBar7);        add_face.setVisibility(View.GONE);        user = UserDB.getAuthUser(getApplicationContext());        result = new SimilarityClassifier.Recognition( -1f);        //Camera Permission        if (checkSelfPermission(Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {            requestPermissions(new String[]{Manifest.permission.CAMERA}, MY_CAMERA_REQUEST_CODE);        }        //On-screen Action Button        //On-screen switch to toggle between Cameras.        camera_switch.setOnClickListener(v -> {            if (cam_face == CameraSelector.LENS_FACING_BACK) {                cam_face = CameraSelector.LENS_FACING_FRONT;                flipX = true;            } else {                cam_face = CameraSelector.LENS_FACING_BACK;                flipX = false;            }            cameraProvider.unbindAll();            cameraBind();        });        add_face.setOnClickListener((v -> addFace()));        add_photo.setOnClickListener((v -> loadPhoto()));        reco_name.setText("Add Face of " + user.getName());        preview_info.setText("\n1.Bring Face in view of Camera.\n2.Your Face preview will appear here.\n3.Click Submit button to save face.");        recognize.setOnClickListener(v -> readyFaceData());        //Load model        try {            tfLite = new Interpreter(loadModelFile(AddFace.this, modelFile));        } catch (IOException e) {            e.printStackTrace();        }        //Initialize Face Detector        FaceDetectorOptions highAccuracyOpts =                new FaceDetectorOptions.Builder()                        .setPerformanceMode(FaceDetectorOptions.PERFORMANCE_MODE_ACCURATE)                        .build();        detector = com.google.mlkit.vision.face.FaceDetection.getClient(highAccuracyOpts);        cameraBind();    }    private void readyFaceData() {        start = true;        add_face.setVisibility(View.VISIBLE);        face_preview.setVisibility(View.VISIBLE);        result.setExtra(embeedings);    }    private void addFace() {        if (result.getExtra() != null) {            loading(true);            registered.put(String.valueOf(user.getId()), result);            String faceData = new Gson().toJson(registered);            Map<String, String> faceDataBody = new HashMap<>();            faceDataBody.put("faceData", faceData);            Log.d("addFace", faceData);            UserDB.setFaceData(getApplicationContext(), faceData);            AuthService authService = ApiUtils.getAuthService(this);            authService.addFace(UserDB.getToken(getApplicationContext()), faceDataBody).enqueue(new retrofit2.Callback() {                @Override                public void onResponse(@NotNull Call call, @NotNull Response response) {                    if (response.isSuccessful()) {                        Log.d("addFace", "Success");                        Toasty.success(context, "Face Added", Toast.LENGTH_SHORT).show();                        Intent intent = new Intent(AddFace.this, MainActivity.class);                        startActivity(intent);                        finish();                    } else {                        Log.d("addFace", "Failed");                        Toasty.error(context, "Failed to add", Toast.LENGTH_SHORT).show();                    }                    loading(false);                }                @Override                public void onFailure(@NotNull Call call, @NotNull Throwable t) {                    Log.d("addFace", "Failed");                    Toasty.error(context, "Failed to add", Toast.LENGTH_SHORT).show();                    loading(false);                }            });        } else {            Toasty.error(context, "Add Image First!", Toast.LENGTH_SHORT).show();        }    }    void loading(boolean loading) {        if (loading) {            add_face.setVisibility(View.GONE);            pBar.setVisibility(View.VISIBLE);        } else {            add_face.setVisibility(View.VISIBLE);            pBar.setVisibility(View.GONE);        }    }    @Override    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {        super.onRequestPermissionsResult(requestCode, permissions, grantResults);        if (requestCode == MY_CAMERA_REQUEST_CODE) {            if (grantResults[0] == PackageManager.PERMISSION_GRANTED) {                Toast.makeText(this, "camera permission granted", Toast.LENGTH_LONG).show();            } else {                Toast.makeText(this, "camera permission denied", Toast.LENGTH_LONG).show();            }        }    }    private MappedByteBuffer loadModelFile(Activity activity, String MODEL_FILE) throws IOException {        AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_FILE);        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());        FileChannel fileChannel = inputStream.getChannel();        long startOffset = fileDescriptor.getStartOffset();        long declaredLength = fileDescriptor.getDeclaredLength();        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);    }    //Bind camera and preview view    private void cameraBind() {        cameraProviderFuture = ProcessCameraProvider.getInstance(this);        previewView = findViewById(R.id.previewView);        cameraProviderFuture.addListener(() -> {            try {                cameraProvider = cameraProviderFuture.get();                bindPreview(cameraProvider);            } catch (ExecutionException | InterruptedException e) {                // No errors need to be handled for this in Future.                // This should never be reached.            }        }, ContextCompat.getMainExecutor(this));    }    void bindPreview(@NonNull ProcessCameraProvider cameraProvider) {        Preview preview = new Preview.Builder()                .build();        cameraSelector = new CameraSelector.Builder()                .requireLensFacing(cam_face)                .build();        preview.setSurfaceProvider(previewView.getSurfaceProvider());        ImageAnalysis imageAnalysis =                new ImageAnalysis.Builder()                        .setTargetResolution(new Size(640, 480))                        .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST) //Latest frame is shown                        .build();        Executor executor = Executors.newSingleThreadExecutor();        imageAnalysis.setAnalyzer(executor, imageProxy -> {            InputImage image = null;            @SuppressLint({"UnsafeExperimentalUsageError", "UnsafeOptInUsageError"})            // Camera Feed-->Analyzer-->ImageProxy-->mediaImage-->InputImage(needed for ML kit face detection)                    Image mediaImage = imageProxy.getImage();            if (mediaImage != null) {                image = InputImage.fromMediaImage(mediaImage, imageProxy.getImageInfo().getRotationDegrees());                System.out.println("Rotation " + imageProxy.getImageInfo().getRotationDegrees());            }            System.out.println("ANALYSIS");            //Process acquired image to detect faces            detector.process(Objects.requireNonNull(image))                    .addOnSuccessListener(                            faces -> {                                if (faces.size() != 0) {                                    Face face = faces.get(0); //Get first face from detected faces                                    System.out.println(face);                                    //mediaImage to Bitmap                                    Bitmap frame_bmp = toBitmap(mediaImage);                                    int rot = imageProxy.getImageInfo().getRotationDegrees();                                    //Adjust orientation of Face                                    Bitmap frame_bmp1 = rotateBitmap(frame_bmp, rot, flipX);                                    //Get bounding box of face                                    RectF boundingBox = new RectF(face.getBoundingBox());                                    //Crop out bounding box from whole Bitmap(image)                                    Bitmap cropped_face = getCropBitmapByCPU(frame_bmp1, boundingBox);                                    //Scale the acquired Face to 112*112 which is required input for model                                    Bitmap scaled = getResizedBitmap(cropped_face, 112, 112);                                    if (start)                                        recognizeImage(scaled); //Send scaled bitmap to create face embeddings.                                    System.out.println(boundingBox);                                    try {                                        Thread.sleep(100);  //Camera preview refreshed every 100 millisec(adjust as required)                                    } catch (InterruptedException e) {                                        e.printStackTrace();                                    }                                }                            })                    .addOnFailureListener(                            e -> {                                // Task failed with an exception                                // ...                            })                    .addOnCompleteListener(task -> {                        imageProxy.close(); //v.important to acquire next frame for analysis                    });        });        cameraProvider.bindToLifecycle(this, cameraSelector, imageAnalysis, preview);    }    @SuppressLint("SetTextI18n")    public void recognizeImage(final Bitmap bitmap) {        // set Face to Preview        face_preview.setImageBitmap(bitmap);        //Create ByteBuffer to store normalized image        ByteBuffer imgData = ByteBuffer.allocateDirect(inputSize * inputSize * 3 * 4);        imgData.order(ByteOrder.nativeOrder());        intValues = new int[inputSize * inputSize];        //get pixel values from Bitmap to normalize        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());        imgData.rewind();        for (int i = 0; i < inputSize; ++i) {            for (int j = 0; j < inputSize; ++j) {                int pixelValue = intValues[i * inputSize + j];                if (isModelQuantized) {                    // Quantized model                    imgData.put((byte) ((pixelValue >> 16) & 0xFF));                    imgData.put((byte) ((pixelValue >> 8) & 0xFF));                    imgData.put((byte) (pixelValue & 0xFF));                } else {                    // Float model                    imgData.putFloat((((pixelValue >> 16) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);                    imgData.putFloat((((pixelValue >> 8) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);                    imgData.putFloat(((pixelValue & 0xFF) - IMAGE_MEAN) / IMAGE_STD);                }            }        }        //imgData is input to our model        Object[] inputArray = {imgData};        Map<Integer, Object> outputMap = new HashMap<>();        embeedings = new float[1][OUTPUT_SIZE]; //output of model will be stored in this variable        outputMap.put(0, embeedings);        tfLite.runForMultipleInputsOutputs(inputArray, outputMap); //Run model    }    //Compare Faces by distance between face embeddings    public Bitmap getResizedBitmap(Bitmap bm, int newWidth, int newHeight) {        int width = bm.getWidth();        int height = bm.getHeight();        float scaleWidth = ((float) newWidth) / width;        float scaleHeight = ((float) newHeight) / height;        // CREATE A MATRIX FOR THE MANIPULATION        Matrix matrix = new Matrix();        // RESIZE THE BIT MAP        matrix.postScale(scaleWidth, scaleHeight);        // "RECREATE" THE NEW BITMAP        Bitmap resizedBitmap = Bitmap.createBitmap(                bm, 0, 0, width, height, matrix, false);        bm.recycle();        return resizedBitmap;    }    private static Bitmap getCropBitmapByCPU(Bitmap source, RectF cropRectF) {        Bitmap resultBitmap = Bitmap.createBitmap((int) cropRectF.width(),                (int) cropRectF.height(), Bitmap.Config.ARGB_8888);        Canvas cavas = new Canvas(resultBitmap);        // draw background        Paint paint = new Paint(Paint.FILTER_BITMAP_FLAG);        paint.setColor(Color.WHITE);        cavas.drawRect(//from  w w  w. ja v  a  2s. c  om                new RectF(0, 0, cropRectF.width(), cropRectF.height()),                paint);        Matrix matrix = new Matrix();        matrix.postTranslate(-cropRectF.left, -cropRectF.top);        cavas.drawBitmap(source, matrix, paint);        if (source != null && !source.isRecycled()) {            source.recycle();        }        return resultBitmap;    }    private static Bitmap rotateBitmap(            Bitmap bitmap, int rotationDegrees, boolean flipX) {        Matrix matrix = new Matrix();        // Rotate the image back to straight.        matrix.postRotate(rotationDegrees);        // Mirror the image along the X or Y axis.        matrix.postScale(flipX ? -1.0f : 1.0f, 1.0f);        Bitmap rotatedBitmap =                Bitmap.createBitmap(bitmap, 0, 0, bitmap.getWidth(), bitmap.getHeight(), matrix, true);        // Recycle the old bitmap if it has changed.        if (rotatedBitmap != bitmap) {            bitmap.recycle();        }        return rotatedBitmap;    }    //IMPORTANT. If conversion not done ,the toBitmap conversion does not work on some devices.    private static byte[] YUV_420_888toNV21(Image image) {        int width = image.getWidth();        int height = image.getHeight();        int ySize = width * height;        int uvSize = width * height / 4;        byte[] nv21 = new byte[ySize + uvSize * 2];        ByteBuffer yBuffer = image.getPlanes()[0].getBuffer(); // Y        ByteBuffer uBuffer = image.getPlanes()[1].getBuffer(); // U        ByteBuffer vBuffer = image.getPlanes()[2].getBuffer(); // V        int rowStride = image.getPlanes()[0].getRowStride();        int pos = 0;        if (rowStride == width) { // likely            yBuffer.get(nv21, 0, ySize);            pos += ySize;        } else {            long yBufferPos = -rowStride; // not an actual position            for (; pos < ySize; pos += width) {                yBufferPos += rowStride;                yBuffer.position((int) yBufferPos);                yBuffer.get(nv21, pos, width);            }        }        rowStride = image.getPlanes()[2].getRowStride();        int pixelStride = image.getPlanes()[2].getPixelStride();        if (pixelStride == 2 && rowStride == width && uBuffer.get(0) == vBuffer.get(1)) {            // maybe V an U planes overlap as per NV21, which means vBuffer[1] is alias of uBuffer[0]            byte savePixel = vBuffer.get(1);            try {                vBuffer.put(1, (byte) ~savePixel);                if (uBuffer.get(0) == (byte) ~savePixel) {                    vBuffer.put(1, savePixel);                    vBuffer.position(0);                    uBuffer.position(0);                    vBuffer.get(nv21, ySize, 1);                    uBuffer.get(nv21, ySize + 1, uBuffer.remaining());                    return nv21; // shortcut                }            } catch (ReadOnlyBufferException ex) {                // unfortunately, we cannot check if vBuffer and uBuffer overlap            }            // unfortunately, the check failed. We must save U and V pixel by pixel            vBuffer.put(1, savePixel);        }        // other optimizations could check if (pixelStride == 1) or (pixelStride == 2),        // but performance gain would be less significant        for (int row = 0; row < height / 2; row++) {            for (int col = 0; col < width / 2; col++) {                int vuPos = col * pixelStride + row * rowStride;                nv21[pos++] = vBuffer.get(vuPos);                nv21[pos++] = uBuffer.get(vuPos);            }        }        return nv21;    }    private Bitmap toBitmap(Image image) {        byte[] nv21 = YUV_420_888toNV21(image);        YuvImage yuvImage = new YuvImage(nv21, ImageFormat.NV21, image.getWidth(), image.getHeight(), null);        ByteArrayOutputStream out = new ByteArrayOutputStream();        yuvImage.compressToJpeg(new Rect(0, 0, yuvImage.getWidth(), yuvImage.getHeight()), 75, out);        byte[] imageBytes = out.toByteArray();        //System.out.println("bytes"+ Arrays.toString(imageBytes));        //System.out.println("FORMAT"+image.getFormat());        return BitmapFactory.decodeByteArray(imageBytes, 0, imageBytes.length);    }    //Load Faces from Shared Preferences.Json String to SimilarityClassifier.Recognition object    private HashMap<String, SimilarityClassifier.Recognition> readFromSP() {        SharedPreferences sharedPreferences = getSharedPreferences("HashMap", MODE_PRIVATE);        String defValue = new Gson().toJson(new HashMap<String, SimilarityClassifier.Recognition>());        String json = sharedPreferences.getString("map", defValue);        // System.out.println("Output json"+json.toString());        TypeToken<HashMap<String, SimilarityClassifier.Recognition>> token = new TypeToken<HashMap<String, SimilarityClassifier.Recognition>>() {        };        HashMap<String, SimilarityClassifier.Recognition> retrievedMap = new Gson().fromJson(json, token.getType());        // System.out.println("Output map"+retrievedMap.toString());        //During type conversion and save/load procedure,format changes(eg float converted to double).        //So embeddings need to be extracted from it in required format(eg.double to float).        for (Map.Entry<String, SimilarityClassifier.Recognition> entry : retrievedMap.entrySet()) {            float[][] output = new float[1][OUTPUT_SIZE];            ArrayList arrayList = (ArrayList) entry.getValue().getExtra();            arrayList = (ArrayList) arrayList.get(0);            for (int counter = 0; counter < arrayList.size(); counter++) {                output[0][counter] = ((Double) arrayList.get(counter)).floatValue();            }            entry.getValue().setExtra(output);            //System.out.println("Entry output "+entry.getKey()+" "+entry.getValue().getExtra() );        }//        System.out.println("OUTPUT"+ Arrays.deepToString(outut));        Toast.makeText(context, "SimilarityClassifier.Recognitions Loaded", Toast.LENGTH_SHORT).show();        return retrievedMap;    }    //Load Photo from phone storage    private void loadPhoto() {        start = false;        Intent intent = new Intent();        intent.setType("image/*");        intent.setAction(Intent.ACTION_GET_CONTENT);        startActivityForResult(Intent.createChooser(intent, "Select Picture"), SELECT_PICTURE);    }    //Similar Analyzing Procedure    public void onActivityResult(int requestCode, int resultCode, Intent data) {        super.onActivityResult(requestCode, resultCode, data);        if (resultCode == RESULT_OK) {            if (requestCode == SELECT_PICTURE) {                Uri selectedImageUri = data.getData();                try {                    InputImage impPhoto = InputImage.fromBitmap(getBitmapFromUri(selectedImageUri), 0);                    detector.process(impPhoto).addOnSuccessListener(faces -> {                        if (faces.size() != 0) {                            add_face.setVisibility(View.VISIBLE);                            face_preview.setVisibility(View.VISIBLE);                            Face face = faces.get(0);                            System.out.println(face);                            //write code to recreate bitmap from source                            //Write code to show bitmap to canvas                            Bitmap frame_bmp = null;                            try {                                frame_bmp = getBitmapFromUri(selectedImageUri);                            } catch (IOException e) {                                e.printStackTrace();                            }                            Bitmap frame_bmp1 = rotateBitmap(frame_bmp, 0, flipX);                            //face_preview.setImageBitmap(frame_bmp1);                            RectF boundingBox = new RectF(face.getBoundingBox());                            Bitmap cropped_face = getCropBitmapByCPU(frame_bmp1, boundingBox);                            Bitmap scaled = getResizedBitmap(cropped_face, 112, 112);                            // face_preview.setImageBitmap(scaled);                            recognizeImage(scaled);                            readyFaceData();                            System.out.println(boundingBox);                            try {                                Thread.sleep(100);                            } catch (InterruptedException e) {                                e.printStackTrace();                            }                        }                    }).addOnFailureListener(e -> {                        start = true;                        Toast.makeText(context, "Failed to add", Toast.LENGTH_SHORT).show();                    });                    face_preview.setImageBitmap(getBitmapFromUri(selectedImageUri));                } catch (IOException e) {                    e.printStackTrace();                }            }        }    }    private Bitmap getBitmapFromUri(Uri uri) throws IOException {        ParcelFileDescriptor parcelFileDescriptor =                getContentResolver().openFileDescriptor(uri, "r");        FileDescriptor fileDescriptor = parcelFileDescriptor.getFileDescriptor();        Bitmap image = BitmapFactory.decodeFileDescriptor(fileDescriptor);        parcelFileDescriptor.close();        return image;    }    @Override    public void onBackPressed() {        new MaterialDialog.Builder(this)                .title("Discard Face Detection?")                .content("Are you sure want to Discard adding FAce?")                .positiveText("Yes")                .canceledOnTouchOutside(false)                .cancelable(false)                .onPositive((dialog, which) -> {                    finish();                    super.onBackPressed();                }).negativeText("No")                .show();    }}